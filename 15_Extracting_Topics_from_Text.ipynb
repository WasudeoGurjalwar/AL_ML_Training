{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WasudeoGurjalwar/AL_ML_Training/blob/main/15_Extracting_Topics_from_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1cTWgmWtPoK"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1n3kK5ev0YR5K51HSrxjnoSRUw7ywwUIL\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_7hiwjKlFzm"
      },
      "source": [
        "Extracting Topics from Text\n",
        "--\n",
        "In this section, we are going to discuss how to identify topics from the\n",
        "document. Say, for example, there is an online library with multiple departments based on the kind of book. As the new book comes in,\n",
        "you want to look at the unique keywords/topics and decide on which\n",
        "department this book might belong to and place it accordingly. In these\n",
        "kinds of situations, topic modeling would be handy.\n",
        "\n",
        "<font color='green'><b>Basically, this is document tagging and clustering. </b></font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKGrNc_DlFzo"
      },
      "source": [
        "Problem\n",
        "--\n",
        "You want to extract or identify topics from the document.\n",
        "\n",
        "Solution\n",
        "--\n",
        "The simplest way to do this by using the gensim library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8DyP7gWlFzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "809446d5-11a6-4b95-b8b8-8b8a3d179a00"
      },
      "source": [
        "# step 1: define some text documents\n",
        "doc1 = \"I am learning NLP, it is very interesting!! and exciting. it includes machine learning and deep learning\"\n",
        "doc2 = \"My father is a data scientist and he is nlp expert\"\n",
        "doc3 = \"My sister has good exposure into android development\"\n",
        "doc_complete = [doc1, doc2, doc3]\n",
        "doc_complete"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am learning NLP, it is very interesting!! and exciting. it includes machine learning and deep learning',\n",
              " 'My father is a data scientist and he is nlp expert',\n",
              " 'My sister has good exposure into android development']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdb0s93o_P_Q"
      },
      "source": [
        "<font color='red'><b>Please Note - IMPORTANT</b></font>\n",
        "\n",
        "You may be wondering, that if I simply find cosine similarity or use fuzzywuzzy package , I can find which documents are similar. Well, thats true when the <font color='green'><b> words in both the documents or their lemma's are same.</b></font>\n",
        "\n",
        "Here the sentence_1 or document_1 has very few common words w.r.t sentence_2 or document_2, But, both can be classified into the <font color='green'><b>TOPIC of DATA SCIENCE</b></font> with a certain degree of confidence. Thus indicating **Topic Modelling** and not merely word matching !!\n",
        "\n",
        "This NB teaches us techniques like <b>LDA</b>  and  <b>LSA</b> which go bound simple word similarity algos like cosine or phonetic algo's.\n",
        "\n",
        "Watch the video once, before moving ahead :\n",
        "\n",
        "<a href=\"https://drive.google.com/open?id=1IoSAsPZKtIKX3_amqZhyXFkoCaMZs1Ch\">\n",
        "<img border=\"0\" alt=\"TopicModellingConcept\" src=\"https://drive.google.com/uc?id=14OOsd0HaKoMJjqu5YT5n7-HsvE6UVV7z\" width=\"100\" height=\"60\">\n",
        "</a>\n",
        "\n",
        "<small><font color='brown'><b>Don't worry about the Maths, its anyways implemented inside the LSA or LDA algo's </b></font></small>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz0R47znlFzz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d8e70fc-74ea-47f4-caca-5f835e114091"
      },
      "source": [
        "# step 2: Cleaning and preprocessing\n",
        "\n",
        "# Install and import libraries\n",
        "# !pip install gensim\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Text preprocessing as discussed in part 3\n",
        "stop = set(stopwords.words('english'))\n",
        "#print(stop)\n",
        "exclude = set(string.punctuation)\n",
        "#print(exclude)\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "def clean(doc):\n",
        " stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
        " #print(stop_free) # Note That : stop_free is a single string, not a list\n",
        " punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
        " normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
        " return normalized\n",
        "\n",
        "doc_clean = [clean(doc).split() for doc in doc_complete]\n",
        "print(doc_clean)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['learning', 'nlp', 'interesting', 'exciting', 'includes', 'machine', 'learning', 'deep', 'learning'], ['father', 'data', 'scientist', 'nlp', 'expert'], ['sister', 'good', 'exposure', 'android', 'development']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NstrHrrMlFz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1828486a-4819-4961-a17e-c7ba1a1adbfb"
      },
      "source": [
        "# step 3: Preparing document term matrix\n",
        "\n",
        "# Importing gensim\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# Creating the term dictionary of our corpus, where every unique term\n",
        "# is assigned an index.\n",
        "dictionary = corpora.Dictionary(doc_clean)\n",
        "print(dictionary)\n",
        "\n",
        "# Converting a list of documents (corpus) into Document-Term Matrix\n",
        "# using dictionary prepared above.\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "doc_term_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary<16 unique tokens: ['deep', 'exciting', 'includes', 'interesting', 'learning']...>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3), (5, 1), (6, 1)],\n",
              " [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
              " [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvGez-iXlF0F"
      },
      "source": [
        "Must watch for LDA (Latent Dirichlet Allocation)\n",
        "----\n",
        "\n",
        "<a href=\"https://drive.google.com/file/d/12a9W1OwNaDBt0O3ADp4724P1uZ6sQuUV/view?usp=sharing\">\n",
        "<img border=\"0\" alt=\"LDATopicModelling\" src=\"https://drive.google.com/uc?id=14OOsd0HaKoMJjqu5YT5n7-HsvE6UVV7z\" width=\"100\" height=\"60\">\n",
        "</a>\n",
        "\n",
        "<small>Credits : LDA concept Video recorded by Andrius Knispelis </small>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VMS2LOblFz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5effa713-b385-4986-cc09-04e282602d37"
      },
      "source": [
        "# Creating the object for LDA model using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "\n",
        "# Running and Training LDA model on the document term matrix for 3 topics.\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word=dictionary, passes=50)\n",
        "\n",
        "# Results\n",
        "print(ldamodel.print_topics())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, '0.173*\"learning\" + 0.069*\"exciting\" + 0.069*\"includes\" + 0.069*\"machine\" + 0.069*\"deep\" + 0.069*\"interesting\" + 0.069*\"android\" + 0.069*\"good\" + 0.069*\"exposure\" + 0.069*\"development\"'), (1, '0.063*\"nlp\" + 0.063*\"sister\" + 0.063*\"good\" + 0.063*\"exposure\" + 0.063*\"development\" + 0.063*\"android\" + 0.062*\"deep\" + 0.062*\"interesting\" + 0.062*\"machine\" + 0.062*\"includes\"'), (2, '0.129*\"nlp\" + 0.129*\"data\" + 0.129*\"father\" + 0.129*\"scientist\" + 0.129*\"expert\" + 0.032*\"good\" + 0.032*\"exposure\" + 0.032*\"development\" + 0.032*\"android\" + 0.032*\"sister\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-aUKoD5RX82"
      },
      "source": [
        "All the weights associated with the topics from the sentence seem almost similar. You can perform this on huge data to extract significant\n",
        "topics. The whole idea to implement this on sample data is to make you familiar with it, and you can use the same code snippet to perform on the\n",
        "huge data for significant results and insights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJq5Fqufx1nZ"
      },
      "source": [
        "Must watch Video Series on **LSA** :\n",
        "\n",
        "> https://www.youtube.com/watch?v=hB51kkus-Rc\n",
        "\n",
        "> https://www.youtube.com/watch?v=Fy0bF7u6W20\n",
        "\n",
        "> https://www.youtube.com/watch?v=NWb_4O3ssbA\n",
        "\n",
        "> https://www.youtube.com/watch?v=YX4xRIQ84Z0\n",
        "\n",
        "And then move on to implementing yourself this **`kaggle NB`**\n",
        "\n",
        "> https://www.kaggle.com/rcushen/topic-modelling-with-lsa-and-lda\n",
        "\n",
        "<font color='green'>\n",
        "On this NB , I would take Vivas to check on your understandibility on \"Topic Modeling\". </font>\n",
        "\n",
        "This would also land as a **`Project`** in your **`resume`**.\n",
        "\n",
        "<br>\n",
        "\n",
        "<b><u>Viva Questions</u> could be some thing like this :</b>\n",
        "\n",
        "> What is gensim ?\n",
        "\n",
        "> What is corpora in gensim ?\n",
        "\n",
        "> use of doc2bow() ?\n",
        "\n",
        "Refer : <a href=\"https://drive.google.com/file/d/1IhE5ZV_R_Q-BvuNvGIdxhJBPq2lEKqOM/view?usp=sharing\"> This video</a> to answer below Qns:\n",
        "> Can u explain the blueprint of LDA model ?\n",
        "\n",
        "> What do dirichlet distribution (alpha and Beta) parameters signify ?\n",
        "\n",
        "> What do multinomial distribution (theta and phi) parameters signify ?\n",
        "\n",
        "> What is Gibbs Sampling in LDA topic modeling ?  \n",
        "Refer <a href='https://drive.google.com/file/d/1pmbaVVZkt5uq4hIaGfekKNntRoV98bBu/view?usp=sharing'>this video</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsSUkIP9lF0F"
      },
      "source": [
        "<hr>\n",
        "\n",
        "**`Recommended (Extra) Reading for all participants`**\n",
        "\n",
        "Introduction\n",
        "> https://monkeylearn.com/blog/introduction-to-topic-modeling/\n",
        "\n",
        "All about Gensim Library with Implementation Code\n",
        "> https://www.machinelearningplus.com/nlp/gensim-tutorial/\n",
        "\n",
        "Codes from the Standford NLP Group\n",
        "> https://nlp.stanford.edu/software/tmt/tmt-0.4/  "
      ]
    }
  ]
}