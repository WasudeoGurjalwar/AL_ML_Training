{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WasudeoGurjalwar/AL_ML_Training/blob/main/Gemini_LLM_Google_python_NB_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API - Usage and Usecases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOxMUKTxR-_j"
      },
      "source": [
        "This Notebook demonstrates how to use the Python SDK for the Gemini API, which gives you access to Google's Gemini large language models. In this notebook, you will learn how to:\n",
        "\n",
        "1. Set up your development environment and API access to use Gemini.\n",
        "2. Generate text responses from text inputs.\n",
        "3. Generate text responses from multimodal inputs (text and images).\n",
        "4. Use Gemini for multi-turn conversations (chat).\n",
        "5. Use embeddings for large language models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFPBKLapSCkM"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFNV1e3ASJha"
      },
      "source": [
        "### Install the Python SDK\n",
        "\n",
        "The Python SDK for the Gemini API, is contained in the [`google-generativeai`](https://pypi.org/project/google-generativeai/) package. Install the dependency using pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OEoeosRTv-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4ea69b-5ae6-4088-9347-a9abc04684dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m160.8/160.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m760.0/760.0 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai\n",
        "## -q option in the pip install command stands for \"quiet\" mode. less verbose\n",
        "## -U -U option stands for \"upgrade,\" which instructs pip to upgrade the specified package to the latest version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCFF5VSTbcAR"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRC2HngneEeQ"
      },
      "source": [
        "Import the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS9l5igubpHO"
      },
      "outputs": [],
      "source": [
        "import pathlib # allows to create a Path object\n",
        "import textwrap # provides functions for formatting and wrapping plain text\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "## This code block serves several purposes:\n",
        "\n",
        "#1. Importing Modules:\n",
        "#   - It imports necessary modules for the code to function: `pathlib`, `textwrap`, `google.generativeai` from the `genai` package, and specific display-related modules from IPython.\n",
        "\n",
        "#2. Defining a Markdown Converter Function (`to_markdown()`):\n",
        "#   - This function, `to_markdown()`, converts a given text into Markdown format.\n",
        "#   - It replaces bullet points represented by 'â€¢' with Markdown syntax '*'.\n",
        "#   - It indents the entire text block with '>' using the `textwrap.indent()` function, making it suitable for Markdown display in IPython.\n",
        "\n",
        "#3. IPython Display Modules:\n",
        "#   - It imports modules from IPython for displaying Markdown (`Markdown`) and other content (`display`), enabling formatted output within an IPython environment.\n",
        "\n",
        "#In short, this code block sets up the necessary environment for working with Google's Generative AI module\n",
        " # (`google.generativeai`), defines a utility function to convert text into Markdown format\n",
        " # with appropriate indentation and bullet point representation,\n",
        " # and imports display modules for output within an IPython environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d10c38a5c91f"
      },
      "outputs": [],
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHYFrFPjSGNq"
      },
      "source": [
        "### Setup your API key\n",
        "\n",
        "Before you can use the Gemini API, you must first obtain an API key. If you don't already have one, create a key with one click in Google AI Studio.\n",
        "\n",
        "<a class=\"button button-primary\" href=\"https://makersuite.google.com/app/apikey\" target=\"_blank\" rel=\"noopener noreferrer\">Get an API key</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHhsUxDTdw0W"
      },
      "source": [
        "In Colab, add the key to the secrets manager under the \"ðŸ”‘\" in the left panel. Give it the name `GOOGLE_API_KEY`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmSlTHXxb5pV"
      },
      "source": [
        "Once you have the API key, pass it to the SDK. You can do this in two ways:\n",
        "\n",
        "* Put the key in the `GOOGLE_API_KEY` environment variable (the SDK will automatically pick it up from there).\n",
        "* Pass the key to `genai.configure(api_key=...)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab9ASynfcIZn"
      },
      "outputs": [],
      "source": [
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ssbTMNVSMd-"
      },
      "source": [
        "## List models\n",
        "\n",
        "Now you're ready to call the Gemini API. Use `list_models` to see the available Gemini models:\n",
        "\n",
        "* `gemini-pro`: optimized for text-only prompts.\n",
        "* `gemini-pro-vision`: optimized for text-and-images prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvvWFy08e5c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "9809f22c-86fe-4027-e3fe-17879867fcd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro\n",
            "models/gemini-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-pro-exp-0801\n",
            "models/gemini-1.5-pro-exp-0827\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-exp-0827\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTl5NjtrhA0J"
      },
      "source": [
        "Note: For detailed information about the available models, including their capabilities and rate limits, see [Gemini models](https://ai.google.dev/models/gemini). There are options for requesting [rate limit increases](https://ai.google.dev/docs/increase_quota). The rate limit for Gemini-Pro models is 60 requests per minute (RPM).\n",
        "\n",
        "The Gemini models support the generic, multimodal capabilities of the `generateContent` method. ( \"multimodal\" refers to the ability of the Gemini models to handle and generate content from multiple modes - such as text, images, audio, or other forms of data )."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZfoK3I3hu6V"
      },
      "source": [
        "## Generate text from text inputs\n",
        "\n",
        "For text-only prompts, use the `gemini-pro` model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bcfnGEviwTI"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-pro')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR_2A_sxk8sK"
      },
      "source": [
        "The `generate_content` method can handle a wide variety of use cases, including multi-turn chat and multimodal input, depending on what the underlying model supports. The available models only support text and images as input, and text as output.\n",
        "\n",
        "In the simplest case, you can pass a prompt string to the <a href=\"https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content\"><code>GenerativeModel.generate_content</code></a> method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he-OfzBbhACQ",
        "outputId": "f2cd0e19-3963-4af2-e7f8-b911f7771950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 127 ms, sys: 14.7 ms, total: 142 ms\n",
            "Wall time: 8.37 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response = model.generate_content(\"What is the meaning of life?\")\n",
        "\n",
        "## \"user 316 ms\": The code spent 316 milliseconds of CPU time in user-mode.\n",
        "## \"sys: 38 ms\": The code spent an additional 38 milliseconds of CPU time in system-mode.\n",
        "## \"total: 354 ms\": The total CPU time spent executing the code is 354 milliseconds.\n",
        "## \"Wall time\": Refers to the actual time elapsed from the start to the end of the code execution,\n",
        "##              as measured by a wall clock.\n",
        "\n",
        "## \"22.2 s\": The code execution took a total of 22.2 seconds of wall clock time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbrR-n_qlpFd"
      },
      "source": [
        "In simple cases, the `response.text` accessor is all you need. To display formatted Markdown text, use the `to_markdown` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-zBkueElVEO",
        "outputId": "455d5365-1fff-426d-fb2b-d70b07c30d60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **Multiple Perspectives on the Meaning of Life:**\n> \n> **Philosophical and Religious Perspectives:**\n> \n> * **Existentialism:** There is no inherent meaning to life; individuals must create their own.\n> * **Stoicism:** The meaning of life is to live virtuously and accept fate with equanimity.\n> * **Utilitarianism:** The meaning of life is to maximize happiness and minimize suffering.\n> * **Buddhism:** The meaning of life is to overcome suffering by following the Eightfold Path.\n> * **Christianity:** The meaning of life is to glorify God, follow Jesus Christ, and live eternity in heaven.\n> \n> **Scientific and Evolutionary Perspectives:**\n> \n> * **Naturalism:** Life has no intrinsic purpose beyond survival and reproduction.\n> * **Sociobiology:** Human behavior is driven by evolutionary adaptations that promote survival and reproductive success.\n> * **Cognitive Psychology:** The meaning of life is a subjective construct created by the human mind, often influenced by culture and personal experiences.\n> \n> **Personal and Subjective Perspectives:**\n> \n> * **Individual Fulfillment:** The meaning of life is to pursue one's passions, develop skills, and create a fulfilling existence.\n> * **Relationships and Connection:** The meaning of life is found in connecting with others, building relationships, and contributing to society.\n> * **Contribution and Legacy:** The meaning of life is to make a positive impact on the world, leave a legacy, and inspire others.\n> \n> **Other Possible Meanings:**\n> \n> * **To Learn and Grow:** The meaning of life is to acquire knowledge, develop wisdom, and continuously evolve as a person.\n> * **To Experience and Appreciate:** The meaning of life is to savor the joys and wonders of the world, appreciate beauty, and find wonder in the present moment.\n> * **To Play and Create:** The meaning of life is to engage in playful activities, express creativity, and bring joy into the world.\n> \n> **Ultimately, the meaning of life is unique to each individual and is shaped by a multitude of factors. It is a personal journey that involves exploration, reflection, and ongoing discovery.**"
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZPpoKMQoru8"
      },
      "source": [
        "If the API failed to return a result, use `GenerateContentRespose.prompt_feedback` to see if it was blocked due to safety concerns regarding the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIQdU8AGoraT",
        "outputId": "1a736f5a-e0e4-4f42-aa98-86cb75af0f40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "response.prompt_feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEJupEDUo6Xj"
      },
      "source": [
        "Gemini can generate multiple possible responses for a single prompt. These possible responses are called `candidates`, and you can review them to select the most suitable one as the response.\n",
        "\n",
        "View the response candidates with <a href=\"https://ai.google.dev/api/python/google/ai/generativelanguage/GenerateContentResponse#candidates\"><code>GenerateContentResponse.candidates</code></a>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoGYz-I7o5wF",
        "outputId": "85b8b07b-4eb0-4c96-cacc-a53597a9d324",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[content {\n",
              "  parts {\n",
              "    text: \"**Multiple Perspectives on the Meaning of Life:**\\n\\n**Philosophical and Religious Perspectives:**\\n\\n* **Existentialism:** There is no inherent meaning to life; individuals must create their own.\\n* **Stoicism:** The meaning of life is to live virtuously and accept fate with equanimity.\\n* **Utilitarianism:** The meaning of life is to maximize happiness and minimize suffering.\\n* **Buddhism:** The meaning of life is to overcome suffering by following the Eightfold Path.\\n* **Christianity:** The meaning of life is to glorify God, follow Jesus Christ, and live eternity in heaven.\\n\\n**Scientific and Evolutionary Perspectives:**\\n\\n* **Naturalism:** Life has no intrinsic purpose beyond survival and reproduction.\\n* **Sociobiology:** Human behavior is driven by evolutionary adaptations that promote survival and reproductive success.\\n* **Cognitive Psychology:** The meaning of life is a subjective construct created by the human mind, often influenced by culture and personal experiences.\\n\\n**Personal and Subjective Perspectives:**\\n\\n* **Individual Fulfillment:** The meaning of life is to pursue one\\'s passions, develop skills, and create a fulfilling existence.\\n* **Relationships and Connection:** The meaning of life is found in connecting with others, building relationships, and contributing to society.\\n* **Contribution and Legacy:** The meaning of life is to make a positive impact on the world, leave a legacy, and inspire others.\\n\\n**Other Possible Meanings:**\\n\\n* **To Learn and Grow:** The meaning of life is to acquire knowledge, develop wisdom, and continuously evolve as a person.\\n* **To Experience and Appreciate:** The meaning of life is to savor the joys and wonders of the world, appreciate beauty, and find wonder in the present moment.\\n* **To Play and Create:** The meaning of life is to engage in playful activities, express creativity, and bring joy into the world.\\n\\n**Ultimately, the meaning of life is unique to each individual and is shaped by a multitude of factors. It is a personal journey that involves exploration, reflection, and ongoing discovery.**\"\n",
              "  }\n",
              "  role: \"model\"\n",
              "}\n",
              "finish_reason: STOP\n",
              "index: 0\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HATE_SPEECH\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HARASSMENT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "response.candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJrwllLnHlBb"
      },
      "source": [
        "By default, the model returns a response after completing the entire generation process. You can also stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated.\n",
        "\n",
        "To stream responses, use <a href=\"https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content\"><code>GenerativeModel.generate_content(..., stream=True)</code></a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7n59b3hHo6-",
        "outputId": "db15f51c-3ed2-4bae-cd9a-e6bdb9695744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 159 ms, sys: 21.9 ms, total: 181 ms\n",
            "Wall time: 9.98 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "response = model.generate_content(\"What is the meaning of life?\", stream=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jt0d0GCIUhg",
        "outputId": "060e2ed4-974f-488c-a59a-70cfd9c19c13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The meaning of life is a deeply personal and philosophical question that has been pondered by\n",
            "________________________________________________________________________________\n",
            " humans throughout history. There is no single definitive answer that satisfies everyone, and different people may find meaning in different aspects of their lives.\n",
            "\n",
            "Some common themes that\n",
            "________________________________________________________________________________\n",
            " emerge when people discuss the meaning of life include:\n",
            "\n",
            "* **Purpose and Fulfillment:** Many people find meaning in pursuing a purpose or goal that gives them a sense of direction and accomplishment. This could involve making a difference in the world, raising a family, or creating something lasting.\n",
            "* **Connection and Relationships:** For\n",
            "________________________________________________________________________________\n",
            " others, meaning comes from their relationships with others, including family, friends, and loved ones. Building strong and meaningful connections can provide a sense of purpose and belonging.\n",
            "* **Personal Growth and Transformation:** Some people find meaning in the journey of personal growth and self-discovery. Through experiences, challenges, and learning, they seek to become better versions of themselves.\n",
            "* **Contribution to Society:** Meaning can also be found in making a positive contribution to society. This could involve volunteering, supporting others, or advocating for causes that one believes in.\n",
            "* **Experiential Appreciation:** Some people find meaning simply in appreciating the present moment\n",
            "________________________________________________________________________________\n",
            " and the beauty of life itself. They may focus on savoring experiences, cultivating gratitude, or connecting with nature.\n",
            "\n",
            "Ultimately, the meaning of life is a subjective experience that is shaped by an individual's values, beliefs, and life circumstances. It is a question that can be explored and re-evaluated throughout one's lifetime, as new experiences and perspectives emerge.\n",
            "\n",
            "Some additional perspectives on the meaning of life:\n",
            "\n",
            "* **Existentialism:** This philosophical school of thought emphasizes the freedom and responsibility of individuals to create their own meaning in life. It suggests that there is no inherent meaning, but that we can find meaning through our actions and choices.\n",
            "* **Buddhism:** Buddhism teaches that life is inherently suffering and impermanent. The goal of Buddhism is to end suffering through detachment, meditation, and the development of wisdom and compassion.\n",
            "* **Stoicism:** Stoicism is a philosophy that focuses on accepting the things we cannot change and living in accordance with nature. It emphasizes the importance of virtue, resilience, and living in the present moment.\n",
            "* **Humanism:** Humanism is a non-religious philosophy that emphasizes the inherent value and potential of human beings. It sees life as an opportunity for growth, learning, and creating a better world.\n",
            "________________________________________________________________________________\n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "for chunk in response:\n",
        "  print(chunk.text)\n",
        "  print(\"_\"*80)\n",
        "\n",
        " ## This is a producer - consumer problem\n",
        " ## as the for loop runs and consumes chunk.text\n",
        " ## the code block \"for chunk in response\" runs to fetch more data from response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b4Hkfj-pm3p"
      },
      "source": [
        "When streaming, some response attributes are not available until you've iterated through all the response chunks. This is demonstrated below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-URRx4chp0Kt"
      },
      "outputs": [],
      "source": [
        "response = model.generate_content(\"What is the meaning of life?\", stream=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HklomMEp9QM"
      },
      "source": [
        "The `prompt_feedback` attribute works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1BvdXjop2V-",
        "outputId": "2a57e279-859c-4126-eff4-3bf41b9de70d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "response.prompt_feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVaFQ4RmqGOH"
      },
      "source": [
        "But attributes like `text` do not:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiRkS6nCqFmM",
        "outputId": "ef2b94db-6fd7-4b5e-b805-d15ac98563a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IncompleteIterationError: Please let the response complete iteration before accessing the final accumulated\n",
            "attributes (or call `response.resolve()`)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  response.text\n",
        "except Exception as e:\n",
        "  print(f'{type(e).__name__}: {e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCzr5ZpNhxLm"
      },
      "source": [
        "## Generate text from image and text inputs\n",
        "\n",
        "Gemini provides a multimodal model (`gemini-pro-vision`) that accepts both text and images as inputs. The `GenerativeModel.generate_content` API is designed to handle multimodal prompts and returns a text output.\n",
        "\n",
        "Let's include an image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtNGTBFF8Pgl",
        "outputId": "f412b8f6-0a4d-4542-da18-19b1b519c009",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  405k  100  405k    0     0  2138k      0 --:--:-- --:--:-- --:--:-- 2143k\n"
          ]
        }
      ],
      "source": [
        "!curl -o image.jpg https://t0.gstatic.com/licensed-image?q=tbn:ANd9GcQ_Kevbk21QBRy-PgB4kQpS79brbmmEG7m3VOTShAn4PecDU5H5UxrJxE3Dw1JiaG17V88QIol19-3TM2wCHw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjnS0vNTsVis",
        "outputId": "d1b6e2fa-057b-4d16-f3ca-e03d6e0f7a0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mThis cell output is too large and can only be displayed while logged in.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import PIL.Image ## PIL.Image refers to the Image module within the Python Imaging Library (PIL)\n",
        "                 ## or its successor, the Pillow library.\n",
        "\n",
        "img = PIL.Image.open('image.jpg')\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r99TN2R8EUD"
      },
      "source": [
        "Use the `gemini-pro-vision` model and pass the image to the model with `generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtXxgVzmJZzE"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-flash')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwYifv298Cj3",
        "outputId": "013445e2-c833-457a-db68-6ca4aee64f05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> This is a photo of two glass containers filled with rice, chicken, broccoli, peppers and carrots. The containers appear to be filled with a healthy meal prepared for meal prepping or to be eaten on the go. There are two chopsticks in the image and the containers are on a grey counter top."
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "response = model.generate_content(img)\n",
        "\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xW2Kyra8pSz"
      },
      "source": [
        "To provide both text and images in a prompt, pass a list containing the strings and images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm9tUYeT8lBc"
      },
      "outputs": [],
      "source": [
        "response = model.generate_content([\"Write a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.\", img], stream=True)\n",
        "response.resolve()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d46826OA9IDS",
        "outputId": "ba33fe8a-a4b5-43c3-b600-800f9ae3bb13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> ## Meal Prep Monday: My Journey to a Healthier Lifestyle\n> \n> I'm all about finding simple ways to improve my life, and that includes taking control of my meals. For me, meal prepping has been a game-changer! It's not just about saving time, it's about ensuring I'm fueling my body with healthy and delicious food. \n> \n> This picture is a perfect example of my meal prep philosophy - a balanced and satisfying meal that's ready to go whenever I am. This delicious container holds a hearty serving of chicken teriyaki, fluffy white rice, colorful bell peppers and carrots, and a healthy dose of broccoli. \n> \n> My journey with meal prepping started with a few simple recipes and a commitment to planning ahead. I've learned a lot along the way, experimenting with different flavors and finding what works best for me. Now, meal prepping is a part of my routine, and it brings me a sense of accomplishment and peace knowing I'm taking care of myself. \n> \n> Are you ready to take the plunge into meal prepping? It might seem daunting, but I promise, it's worth it! Start small, choose recipes you love, and don't be afraid to get creative. And remember, it's a journey, not a race. So, let's make meal prepping fun and delicious! \n"
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsIZmCYVTDHD"
      },
      "source": [
        "## Chat conversations\n",
        "\n",
        "Gemini enables you to have freeform conversations across multiple turns. The `ChatSession` class simplifies the process by managing the state of the conversation, so unlike with `generate_content`, you do not have to store the conversation history as a list.\n",
        "\n",
        "Initialize the chat:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8B9Mwo-TCr2",
        "outputId": "af18f418-4e5f-4b9b-e809-459a24262667",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatSession(\n",
              "    model=genai.GenerativeModel(\n",
              "        model_name='models/gemini-1.5-pro',\n",
              "        generation_config={},\n",
              "        safety_settings={},\n",
              "        tools=None,\n",
              "        system_instruction=None,\n",
              "        cached_content=None\n",
              "    ),\n",
              "    history=[]\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-pro')\n",
        "chat = model.start_chat(history=[])\n",
        "chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Il02N-km9j"
      },
      "source": [
        "Note: The vision model `gemini-pro-vision` is not optimized for multi-turn chat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5odluV7kKbgr"
      },
      "source": [
        "The `ChatSession.send_message` method returns the same `GenerateContentResponse` type as <a href=\"https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content\"><code>GenerativeModel.generate_content</code></a>. It also appends your message and the response to the chat history:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b72zbOEjKRxP",
        "outputId": "6700f088-5b72-46e8-8e33-534400ae9352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> A computer is like a very fast helper that follows your instructions to play games, draw pictures, and learn new things! \n"
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "response = chat.send_message(\"In one sentence, explain how a computer works to a young child.\")\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-5HS2bTOTU9",
        "outputId": "550ebcf7-a19f-4acc-9420-5e86730dc907",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[parts {\n",
              "   text: \"In one sentence, explain how a computer works to a young child.\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"A computer is like a very fast helper that follows your instructions to play games, draw pictures, and learn new things! \\n\"\n",
              " }\n",
              " role: \"model\"]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "chat.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JaiFSIvOcVb"
      },
      "source": [
        "You can keep sending messages to continue the conversation. Use the `stream=True` argument to stream the chat:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxku7mzSObfZ",
        "outputId": "e3ee43d9-c217-4cf7-d9e6-916d49f2a2d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "________________________________________________________________________________\n",
            " computer takes in information (like typing on a keyboard or clicking a mouse), processes\n",
            "________________________________________________________________________________\n",
            " it using a set of instructions called a program, and then outputs the results (\n",
            "________________________________________________________________________________\n",
            "like displaying something on the screen or playing a sound).  It does this all incredibly fast using electrical signals that represent data as 1s and 0s\n",
            "________________________________________________________________________________\n",
            ", which the computer can understand and act upon. \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\"Okay, how about a more detailed explanation to a high schooler?\", stream=True)\n",
        "\n",
        "for chunk in response:\n",
        "  print(chunk.text)\n",
        "  print(\"_\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwCqtZ6D4kvk"
      },
      "source": [
        "`glm.Content` objects contain a list of `glm.Part` objects that each contain either a text (string) or inline_data (`glm.Blob`), where a blob contains binary data and a `mime_type`. The chat history is available as a list of `glm.Content` objects in `ChatSession.history`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvyTmbC2d0k3",
        "outputId": "a457fd85-3195-45ff-b9b8-d0eb07663648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **user**: In one sentence, explain how a computer works to a young child."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **model**: A computer is like a very fast helper that follows your instructions to play games, draw pictures, and learn new things! \n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **user**: Okay, how about a more detailed explanation to a high schooler?"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **model**: A computer takes in information (like typing on a keyboard or clicking a mouse), processes it using a set of instructions called a program, and then outputs the results (like displaying something on the screen or playing a sound).  It does this all incredibly fast using electrical signals that represent data as 1s and 0s, which the computer can understand and act upon. \n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for message in chat.history:\n",
        "  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))\n",
        "\n",
        " ## glm stands for Generic Language Model, here also called Gemini Language Model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9bU0J3vUIbz"
      },
      "source": [
        "## Use embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpHIRU5bj7aW"
      },
      "source": [
        "[Embedding](https://developers.google.com/machine-learning/glossary#embedding-vector) is a technique used to represent information as a list of floating point numbers in an array. With Gemini, you can represent text (words, sentences, and blocks of text) in a vectorized form, making it easier to compare and contrast embeddings. For example, two texts that share a similar subject matter or sentiment should have similar embeddings, which can be identified through mathematical comparison techniques such as cosine similarity.\n",
        "\n",
        "Use the `embed_content` method to generate embeddings. The method handles embedding for the following tasks (`task_type`):\n",
        "\n",
        "Task Type | Description\n",
        "---       | ---\n",
        "RETRIEVAL_QUERY\t| Specifies the given text is a query in a search/retrieval setting.\n",
        "RETRIEVAL_DOCUMENT | Specifies the given text is a document in a search/retrieval setting. Using this task type requires a `title`.\n",
        "SEMANTIC_SIMILARITY\t| Specifies the given text will be used for Semantic Textual Similarity (STS).\n",
        "CLASSIFICATION\t| Specifies that the embeddings will be used for classification.\n",
        "CLUSTERING\t| Specifies that the embeddings will be used for clustering.\n",
        "\n",
        "The following generates an embedding for a single string for document retrieval:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hskqSKnJUHvp",
        "outputId": "c3ffa281-18d0-4d06-c4c4-f8f758885240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.003216741, -0.013358698, -0.017649598, -0.009181086, 0.03926703, 0.00038724372, 0.04898349, -0.0 ... TRIMMED]\n"
          ]
        }
      ],
      "source": [
        "result = genai.embed_content(\n",
        "    model=\"models/embedding-001\",\n",
        "    content=\"What is the meaning of life?\",\n",
        "    task_type=\"retrieval_document\",\n",
        "    title=\"Embedding of single string\")\n",
        "\n",
        "# 1 input -> 1 vector output\n",
        "print(str(result['embedding'])[:100], '... TRIMMED]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcSc3KfflBCQ"
      },
      "source": [
        "Note: The `retrieval_document` task type is the only task that accepts a title.\n",
        "\n",
        "To handle batches of strings, pass a list of strings in `content`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnyD-Joik8LE",
        "outputId": "9f5dab4c-5564-4e5d-c1ed-30d508cec7db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0040260437, 0.004124458, -0.014209415, -0.0018330715, 0.038075767, 0.009535844, 0.0470719, 0.004331602, 0.0076015256, 0.013112175, -0.027572198, -0 ... TRIMMED ...\n",
            "[-0.004049845, -0.0075574904, -0.0073463684, -0.036971524, 0.057347655, 0.0048357504, 0.042623978, -0.027788697, 0.047589146, 0.028451372, -0.00253998 ... TRIMMED ...\n",
            "[0.025310587, -0.0080734305, -0.029902633, 0.011606856, 0.023667773, 0.021672552, 0.04903862, -0.00754809, 0.032922402, 0.036271214, -0.0044310773, -0 ... TRIMMED ...\n"
          ]
        }
      ],
      "source": [
        "result = genai.embed_content(\n",
        "    model=\"models/embedding-001\",\n",
        "    content=[\n",
        "      'What is the meaning of life?',\n",
        "      'How much wood would a woodchuck chuck?',\n",
        "      'How does the brain work?'],\n",
        "    task_type=\"retrieval_document\",\n",
        "    title=\"Embedding of list of strings\")\n",
        "\n",
        "# A list of inputs -> A list of vectors output\n",
        "for v in result['embedding']:\n",
        "  print(str(v)[:150], '... TRIMMED ...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBg0eNeml3d4"
      },
      "source": [
        "While the `genai.embed_content` function accepts simple strings or lists of strings, it is actually built around the `glm.Content` type (like <a href=\"https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content\"><code>GenerativeModel.generate_content</code></a>). `glm.Content` objects are the primary units of conversation in the API.\n",
        "\n",
        "While the `glm.Content` object is multimodal, the `embed_content` method only supports text embeddings. This design gives the API the *possibility* to expand to multimodal embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-wmapZznXrm",
        "outputId": "faa15e2b-ed02-4071-a653-b5159641f51b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "parts {\n",
              "  text: \"A computer takes in information (like typing on a keyboard or clicking a mouse), processes it using a set of instructions called a program, and then outputs the results (like displaying something on the screen or playing a sound).  It does this all incredibly fast using electrical signals that represent data as 1s and 0s, which the computer can understand and act upon. \\n\"\n",
              "}\n",
              "role: \"model\""
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "response.candidates[0].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvX5jsrcnufk",
        "outputId": "3489f024-916a-4099-d3d6-b2e0af66431a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.031727016, -0.046894353, 0.022383668, 0.030764 ... TRIMMED ...\n"
          ]
        }
      ],
      "source": [
        "result = genai.embed_content(\n",
        "    model = 'models/embedding-001',\n",
        "    content = response.candidates[0].content)\n",
        "\n",
        "# 1 input -> 1 vector output\n",
        "print(str(result['embedding'])[:50], '... TRIMMED ...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU8juHCxoUKG"
      },
      "source": [
        "Similarly, the chat history contains a list of `glm.Content` objects, which you can pass directly to the `embed_content` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur5ajPsdnCON",
        "outputId": "96743ea0-e62c-4d63-92be-427b3c7241e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[parts {\n",
              "   text: \"In one sentence, explain how a computer works to a young child.\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"A computer is like a very fast helper that follows your instructions to play games, draw pictures, and learn new things! \\n\"\n",
              " }\n",
              " role: \"model\",\n",
              " parts {\n",
              "   text: \"Okay, how about a more detailed explanation to a high schooler?\"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"A computer takes in information (like typing on a keyboard or clicking a mouse), processes it using a set of instructions called a program, and then outputs the results (like displaying something on the screen or playing a sound).  It does this all incredibly fast using electrical signals that represent data as 1s and 0s, which the computer can understand and act upon. \\n\"\n",
              " }\n",
              " role: \"model\"]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "chat.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3xDB1hwof96",
        "outputId": "8bb2e43a-1faf-45e3-cb28-a0167f0a726b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.014632266, -0.042202696, -0.015757175, 0.01548 ... TRIMMED...\n",
            "[0.009457108, -0.023812458, -0.0053241225, 0.01475 ... TRIMMED...\n",
            "[-0.010055617, -0.07208932, -0.00011750793, -0.023 ... TRIMMED...\n",
            "[-0.031727016, -0.046894353, 0.022383668, 0.030764 ... TRIMMED...\n"
          ]
        }
      ],
      "source": [
        "result = genai.embed_content(\n",
        "    model = 'models/embedding-001',\n",
        "    content = chat.history)\n",
        "\n",
        "# 1 input -> 1 vector output\n",
        "for i,v in enumerate(result['embedding']):\n",
        "  print(str(v)[:50], '... TRIMMED...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuz9-TWDzdlb"
      },
      "source": [
        "## Advanced use cases\n",
        "\n",
        "The following sections discuss advanced use cases and lower-level details of the Python SDK for the Gemini API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5FWJPSD1qFE"
      },
      "source": [
        "### Safety settings\n",
        "\n",
        "The `safety_settings` argument lets you configure what the model blocks and allows in both prompts and responses. By default, safety settings block content with medium and/or high probability of being unsafe content across all dimensions. Learn more about [Safety settings](https://ai.google.dev/docs/safety_setting).\n",
        "\n",
        "Enter a questionable prompt and run the model with the default safety settings, and it will not return any candidates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR1fp12I1yH0",
        "outputId": "268352d5-cb79-4b89-8482-af60228d82ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[content {\n",
              "  parts {\n",
              "    text: \"It is not accurate or helpful to make blanket statements comparing the well-being of entire racial groups.  It\\'s crucial to understand that:\\n\\n* **Systemic racism and historical disadvantages play a significant role:** The legacy of slavery, segregation, and ongoing discrimination has created systemic barriers for Black Americans in areas like education, housing, healthcare, and employment. This has led to significant disparities in wealth, opportunity, and overall well-being.\\n* **Generalizations are harmful:**  Painting any group with a broad brush ignores the vast diversity of experiences within both Black and White communities. There are individuals and families within each group who are thriving and struggling for a variety of reasons. \\n* **Focusing on individual effort alone ignores systemic barriers:** While individual choices and actions are important, they cannot overcome deeply ingrained societal structures that disadvantage certain groups.\\n\\n**Instead of asking why one group is \\\"doing better\\\" than another, it\\'s more constructive to ask:**\\n\\n* **How can we address systemic racism and create equitable opportunities for all?**\\n* **What specific challenges do Black Americans face, and how can we work together to overcome them?** \\n* **How can we build a society that values and supports the success of all its members, regardless of race?**\\n\\nLet\\'s focus on understanding the complex realities of race and inequality in the US, and working towards a more just and equitable society for all. \\n\"\n",
              "  }\n",
              "  role: \"model\"\n",
              "}\n",
              "finish_reason: STOP\n",
              "index: 0\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HATE_SPEECH\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_HARASSMENT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "safety_ratings {\n",
              "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
              "  probability: NEGLIGIBLE\n",
              "}\n",
              "]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "response = model.generate_content('Why are White people doing better than Black in the USA')\n",
        "response.candidates\n",
        "\n",
        "## Please note : index: 0 indicates the position of this response in the sequence of generated outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31Q8kAItGLOU"
      },
      "source": [
        "The `prompt_feedback` will tell you which safety filter blocked the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMUvWNkZ11x4",
        "outputId": "230ee667-0f2c-4076-b941-5219275e9b97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "response.prompt_feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtPC1Fo514ec"
      },
      "source": [
        "Now provide the same prompt to the model with newly configured safety settings, and you may get a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UIt5LKp16jL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "13161ac6-b19d-4b75-e531-76fddb1fbbf8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It's not accurate or fair to make blanket statements about entire racial groups.  It's crucial to understand that:\\n\\n* **Systemic racism and historical disadvantages:** The United States has a long history of slavery, segregation, and discrimination against Black people. These systems have created significant barriers to opportunity that persist today, affecting things like wealth accumulation, access to education, housing, and healthcare.\\n* **Socioeconomic disparities are not due to individual failings:**  Attributing disparities solely to individual effort ignores the deeply ingrained systemic factors at play. \\n* **Generalizations are harmful:** Making sweeping statements about any racial group perpetuates stereotypes and ignores the diversity and individual experiences within those groups.\\n\\n**Here are some areas where systemic racism continues to affect Black Americans:**\\n\\n* **Wealth and Income:** The racial wealth gap is vast. Generational wealth, often built through homeownership and inheritance, has been denied to many Black families due to discriminatory practices.\\n* **Education:**  Black students are more likely to attend underfunded schools and face harsher discipline. \\n* **Criminal Justice System:**  Black Americans are disproportionately arrested, convicted, and incarcerated. \\n* **Healthcare:** Black Americans experience higher rates of chronic illnesses and infant mortality due to disparities in access to quality healthcare and other factors.\\n\\n**Instead of making harmful generalizations, it's important to:**\\n\\n* **Educate ourselves** about the history of racism and its ongoing impact.\\n* **Challenge our own biases** and prejudices.\\n* **Support policies** that promote racial equity and justice.\\n* **Amplify the voices** of Black people and other marginalized groups.\\n\\nBy understanding the systemic nature of racial disparities, we can work towards dismantling these systems and creating a more just and equitable society for all. \\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "response = model.generate_content('Why are White people doing better than Black in the USA',\n",
        "                                  safety_settings={'HATE_SPEECH':'block_none'})\n",
        "response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE_f5EruGUnj"
      },
      "source": [
        "Also note that each candidate has its own `safety_ratings`, in case the prompt passes but the individual responses fail the safety checks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipa-8leY6wsK"
      },
      "source": [
        "### Encode messages -- Additional | Optional Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r47nsUOn6YY"
      },
      "source": [
        "The previous sections relied on the SDK to make it easy for you to send prompts to the API. This section offers a fully-typed equivalent to the previous example, so you can better understand the lower-level details regarding how the SDK encodes messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fthdIItnqki"
      },
      "source": [
        "Underlying the Python SDK is the <a href=\"https://ai.google.dev/api/python/google/ai/generativelanguage\"><code>google.ai.generativelanguage</code></a> client library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6aafWECnpX6"
      },
      "outputs": [],
      "source": [
        "import google.ai.generativelanguage as glm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm1RWcB3n_n0"
      },
      "source": [
        "The SDK attempts to convert your message to a `glm.Content` object, which contains a list of `glm.Part` objects that each contain either:\n",
        "\n",
        "1. a <a href=\"https://www.tensorflow.org/text/api_docs/python/text\"><code>text</code></a> (string)\n",
        "2. `inline_data` (`glm.Blob`), where a blob contains binary `data` and a `mime_type`.\n",
        "\n",
        "You can also pass any of these classes as an equivalent dictionary.\n",
        "\n",
        "Note: The only accepted mime types are some image types, `image/*`.\n",
        "\n",
        "So, the fully-typed equivalent to the previous example is:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqFXdgDFRvlU"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "response = model.generate_content(\n",
        "    glm.Content(\n",
        "        parts = [\n",
        "            glm.Part(text=\"Write a short, engaging blog post based on this picture.\"),\n",
        "            glm.Part(\n",
        "                inline_data=glm.Blob(\n",
        "                    mime_type='image/jpeg',\n",
        "                    data=pathlib.Path('image.jpg').read_bytes()\n",
        "                )\n",
        "            ),\n",
        "        ],\n",
        "    ),\n",
        "    stream=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKithEbeRzDX",
        "outputId": "5cb4f6dc-2cc0-40d0-e672-cd1920580874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> ##  Meal Prep Made Easy: Teriyaki Chicken & Veggie Bowls\n> \n> Who says healthy eating has to be boring? ... [TRIMMED] ..."
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "response.resolve()\n",
        "\n",
        "to_markdown(response.text[:100] + \"... [TRIMMED] ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBqknExlzn0k"
      },
      "source": [
        "### Multi-turn conversations\n",
        "\n",
        "While the `genai.ChatSession` class shown earlier can handle many use cases, it does make some assumptions. If your use case doesn't fit into this chat implementation it's good to remember that `genai.ChatSession` is just a wrapper around <a href=\"https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content\"><code>GenerativeModel.generate_content</code></a>. In addition to single requests, it can handle multi-turn conversations.\n",
        "\n",
        "The individual messages are `glm.Content` objects or compatible dictionaries, as seen in previous sections. As a dictionary, the message requires `role` and `parts` keys. The `role` in a conversation can either be the `user`, which provides the prompts, or `model`, which provides the responses.\n",
        "\n",
        "Pass a list of `glm.Content` objects and it will be treated as multi-turn chat:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtfwMa0HzvZL",
        "outputId": "f9b5c08d-61c7-4c69-885e-fe784211065b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Imagine a computer like a very smart toy box! \n> \n> * It has a special brain called a **CPU** that helps it think and solve problems. \n> * It has a **memory** like you, to remember things like stories and games. \n> * It uses a **hard drive** like a giant backpack to store all its toys, like pictures, videos, and games. \n> \n> When you press a key on the **keyboard** or click the **mouse**, you're talking to the computer. The **screen** shows you what the computer is thinking! \n> \n> So basically, you give the computer instructions, it uses its brain and memory to understand them, and then it shows you the results on the screen. Pretty cool, right? \n"
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-pro')\n",
        "\n",
        "messages = [\n",
        "    {'role':'user',\n",
        "     'parts': [\"Briefly explain how a computer works to a young child.\"]}\n",
        "]\n",
        "response = model.generate_content(messages)\n",
        "\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mqqiDJvzyac"
      },
      "source": [
        "To continue the conversation, add the response and another message.\n",
        "\n",
        "Note: For multi-turn conversations, you need to send the whole conversation history with each request. The API is **stateless**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBxsZBxcz5Ik",
        "outputId": "885a2747-f3a2-45cc-8537-976b8b2a2ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Think of a computer like a layered cake, with each layer doing a specific job:\n> \n> **1. Hardware: The Cake's Ingredients**\n> \n> * **CPU (Central Processing Unit):** The \"brain\" that executes instructions from software. Imagine it as a super-fast calculator that can also make decisions.\n> * **RAM (Random Access Memory):**  The computer's short-term memory. It stores data the CPU needs quickly. Think of it like your desk where you keep things you're working on right now.\n> * **Hard Drive:**  Long-term storage for all your files, like a giant filing cabinet. This is where your operating system, programs, and documents live even when the computer is off.\n> * **Input/Output Devices:**  How you interact with the computer. This includes things like the keyboard, mouse, monitor, printer, etc.\n> \n> **2. Software: The Cake's Recipe**\n> \n> * **Operating System (OS):** The master program that manages all the hardware and software. It's like the conductor of an orchestra, making sure everything works together harmoniously. Windows and macOS are examples of operating systems. \n> * **Applications/Programs:**  These are the tools you use on your computer, like web browsers, word processors, games, etc. Think of them as the different things you can do with the computer.\n> \n> **How It All Works Together:**\n> \n> 1. When you interact with a program (like clicking a button), you send instructions to the CPU.\n> 2. The CPU fetches the necessary data from RAM or the hard drive.\n> 3. The CPU processes the instructions and data, performing calculations or making decisions.\n> 4. The results are sent back to the program, which then displays them on your screen or sends them to another device like a printer. \n> \n> **It's all about communication!**  The hardware provides the physical components, and the software tells the hardware what to do. This constant flow of information and instructions is what makes a computer work. \n"
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "messages.append({'role':'model',\n",
        "                 'parts':[response.text]})\n",
        "\n",
        "messages.append({'role':'user',\n",
        "                 'parts':[\"Okay, how about a more detailed explanation to a high school student?\"]})\n",
        "\n",
        "response = model.generate_content(messages)\n",
        "\n",
        "to_markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qt6Yj2JRf-0"
      },
      "source": [
        "## What's next\n",
        "\n",
        "-   Prompt design is the process of creating prompts that elicit the desired response from language models. Writing well structured prompts is an essential part of ensuring accurate, high quality responses from a language model. Learn about best practices for [prompt writing](https://ai.google.dev/docs/prompt_best_practices).\n",
        "-   Gemini offers several model variations to meet the needs of different use cases, such as input types and complexity, implementations for chat or other dialog language tasks, and size constraints. Learn about the available [Gemini models](https://ai.google.dev/models/gemini).\n",
        "-   Gemini offers options for requesting [rate limit increases](https://ai.google.dev/docs/increase_quota). The rate limit for Gemini-Pro models is 60 requests per minute (RPM)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "google": {
      "image_path": "/static/site-assets/images/docs/logo-python.svg",
      "keywords": [
        "examples",
        "gemini",
        "beginner",
        "googleai",
        "quickstart",
        "python",
        "text",
        "chat",
        "vision",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}