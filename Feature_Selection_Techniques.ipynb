{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WasudeoGurjalwar/AL_ML_Training/blob/main/Feature_Selection_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection is the process of selecting a subset of relevant features (variables or columns) from your dataset to improve the performance of your machine learning model and reduce dimensionality. Here are three common ways to do feature selection in Python with small code samples using the scikit-learn library:\n",
        "\n",
        "1. **Variance Threshold**:\n",
        "\n",
        "   This method removes features with low variance, as they are likely to contain little information. It's suitable for numerical features.\n",
        "\n",
        "   ```python\n",
        "   from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "   # Create a VarianceThreshold instance with a threshold value\n",
        "   selector = VarianceThreshold(threshold=0.1)\n",
        "\n",
        "   # Fit and transform the selector on your dataset\n",
        "   X_new = selector.fit_transform(X)\n",
        "\n",
        "   # X_new will contain only features with variance greater than 0.1\n",
        "   ```\n",
        "\n",
        "2. **SelectKBest with Mutual Information**:\n",
        "\n",
        "   This method selects the top k features based on their mutual information with the target variable. It's suitable for both numerical and categorical features.\n",
        "\n",
        "   ```python\n",
        "   from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "\n",
        "   # Create a SelectKBest instance with mutual information as the scoring function\n",
        "   selector = SelectKBest(score_func=mutual_info_classif, k=5)\n",
        "\n",
        "   # Fit and transform the selector on your dataset\n",
        "   X_new = selector.fit_transform(X, y)\n",
        "\n",
        "   # X_new will contain the top 5 features with the highest mutual information\n",
        "   ```\n",
        "\n",
        "3. **Recursive Feature Elimination (RFE)**:\n",
        "\n",
        "   This method recursively removes the least important features based on the model's performance. It's suitable for any supervised learning problem.\n",
        "\n",
        "   ```python\n",
        "   from sklearn.feature_selection import RFE\n",
        "   from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "   # Create an estimator (e.g., LogisticRegression) and a RFE instance\n",
        "   estimator = LogisticRegression()\n",
        "   selector = RFE(estimator, n_features_to_select=3)\n",
        "\n",
        "   # Fit the selector on your dataset\n",
        "   selector = selector.fit(X, y)\n",
        "\n",
        "   # X_new will contain the top 3 features selected by RFE\n",
        "   ```\n",
        "\n",
        "These are just a few examples of feature selection methods in scikit-learn. Depending on your specific dataset and problem, you can choose the most appropriate method."
      ],
      "metadata": {
        "id": "mgLFSeGL1X6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1 - Variance Threshold feature selection\n",
        "\n",
        "Demonstrates the use of the Variance Threshold feature selection method with a predefined dataset from the scikit-learn library:"
      ],
      "metadata": {
        "id": "-lmwp3sk4Kiu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYnsXvOt1P4d",
        "outputId": "5efe2338-5c3d-4c8f-9fa8-b763ff78c11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n",
            "------------------\n",
            "[0 2 3]\n",
            "------------------\n",
            "\n",
            "Selected Features:\n",
            "   sepal length (cm)  petal length (cm)  petal width (cm)\n",
            "0                5.1                1.4               0.2\n",
            "1                4.9                1.4               0.2\n",
            "2                4.7                1.3               0.2\n",
            "3                4.6                1.5               0.2\n",
            "4                5.0                1.4               0.2\n",
            "\n",
            "Feature Variances:\n",
            "sepal length (cm)    0.685694\n",
            "petal length (cm)    3.116278\n",
            "petal width (cm)     0.581006\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# Load the Iris dataset as an example\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Create a DataFrame from the dataset\n",
        "df = pd.DataFrame(data=X, columns=iris.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"Original Dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Instantiate the VarianceThreshold selector with a threshold value\n",
        "selector = VarianceThreshold(threshold=0.2)\n",
        "\n",
        "# Fit the selector on the dataset and transform it\n",
        "X_new = selector.fit_transform(X)\n",
        "\n",
        "# Get the selected feature indices\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "print(\"------------------\")\n",
        "print(selected_indices)\n",
        "print(\"------------------\")\n",
        "\n",
        "# Create a DataFrame with the selected features\n",
        "df_new = df.iloc[:, selected_indices]\n",
        "\n",
        "# Display the selected features\n",
        "print(\"\\nSelected Features:\")\n",
        "print(df_new.head())\n",
        "\n",
        "# Display the feature variances\n",
        "print(\"\\nFeature Variances:\")\n",
        "print(df.iloc[:, selected_indices].var())\n",
        "\n",
        "\n",
        "##  the variance of a feature can be above 1.\n",
        "## In fact, the variance of a feature measures how much the values of that feature\n",
        "## vary from the mean. If the values of the feature are widely spread out from the mean,\n",
        "## the variance will be higher."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2 - SelectKBest"
      ],
      "metadata": {
        "id": "fvk1w6gl5mBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset as an example\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Create a DataFrame from the dataset\n",
        "df = pd.DataFrame(data=X, columns=data.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"Original Dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Instantiate the SelectKBest selector with mutual information as the scoring function\n",
        "selector = SelectKBest(score_func=mutual_info_classif, k=5)\n",
        "\n",
        "# Fit and transform the selector on the training data\n",
        "X_train_new = selector.fit_transform(X_train, y_train)\n",
        "\n",
        "# Get the indices of the selected features\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "print(selected_indices)\n",
        "\n",
        "# Create a DataFrame with the selected features\n",
        "df_selected = df.iloc[:, selected_indices]\n",
        "\n",
        "# Display the selected features\n",
        "print(\"\\nSelected Features:\")\n",
        "print(df_selected.head())\n",
        "\n",
        "# Train a RandomForestClassifier on the selected features\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train_new, y_train)\n",
        "#clf.fit(X_train, y_train)\n",
        "\n",
        "# Transform the test data using the same selector\n",
        "X_test_new = selector.transform(X_test)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = clf.predict(X_test_new)\n",
        "#y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and display the accuracy of the model on the test data\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nAccuracy on Test Data:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP7pKoCs4UKN",
        "outputId": "09b57ca4-c4ec-44bf-f51b-6cb39afbdfc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "3           0.28390          0.2414              0.10520         0.2597   \n",
            "4           0.13280          0.1980              0.10430         0.1809   \n",
            "\n",
            "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
            "0                 0.07871  ...          17.33           184.60      2019.0   \n",
            "1                 0.05667  ...          23.41           158.80      1956.0   \n",
            "2                 0.05999  ...          25.53           152.50      1709.0   \n",
            "3                 0.09744  ...          26.50            98.87       567.7   \n",
            "4                 0.05883  ...          16.67           152.20      1575.0   \n",
            "\n",
            "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
            "0            0.1622             0.6656           0.7119                0.2654   \n",
            "1            0.1238             0.1866           0.2416                0.1860   \n",
            "2            0.1444             0.4245           0.4504                0.2430   \n",
            "3            0.2098             0.8663           0.6869                0.2575   \n",
            "4            0.1374             0.2050           0.4000                0.1625   \n",
            "\n",
            "   worst symmetry  worst fractal dimension  target  \n",
            "0          0.4601                  0.11890       0  \n",
            "1          0.2750                  0.08902       0  \n",
            "2          0.3613                  0.08758       0  \n",
            "3          0.6638                  0.17300       0  \n",
            "4          0.2364                  0.07678       0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "[ 7 20 22 23 27]\n",
            "\n",
            "Selected Features:\n",
            "   mean concave points  worst radius  worst perimeter  worst area  \\\n",
            "0              0.14710         25.38           184.60      2019.0   \n",
            "1              0.07017         24.99           158.80      1956.0   \n",
            "2              0.12790         23.57           152.50      1709.0   \n",
            "3              0.10520         14.91            98.87       567.7   \n",
            "4              0.10430         22.54           152.20      1575.0   \n",
            "\n",
            "   worst concave points  \n",
            "0                0.2654  \n",
            "1                0.1860  \n",
            "2                0.2430  \n",
            "3                0.2575  \n",
            "4                0.1625  \n",
            "\n",
            "Accuracy on Test Data: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`SelectKBest` in scikit-learn provides several scoring functions that you can use for feature selection. These functions measure the relationship between features and the target variable differently. Some commonly used scoring functions include:\n",
        "\n",
        "1. **f_classif**: This function computes the ANOVA F-statistic for classification tasks. It assesses whether there are significant differences in feature distributions between classes.\n",
        "\n",
        "2. **chi2**: The chi-squared (chi2) statistic is used for feature selection with categorical target variables (classification tasks). It measures the dependence between the feature and the target.\n",
        "\n",
        "3. **f_regression**: This function calculates the F-statistic for regression tasks. It measures the linear relationship between each feature and the target variable.\n",
        "\n",
        "4. **mutual_info_classif**: As previously mentioned, this function calculates the mutual information between features and the target variable for classification tasks. It assesses the dependency and information gain.\n",
        "\n",
        "5. **mutual_info_regression**: Similar to `mutual_info_classif`, this function calculates mutual information but is designed for regression tasks.\n",
        "\n",
        "6. **SelectPercentile**: This method selects a fixed percentage of the highest-scoring features based on the chosen scoring function.\n",
        "\n",
        "Here's an example of how to use `SelectKBest` with the `f_classif` scoring function:\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Instantiate the SelectKBest selector with f_classif as the scoring function and k=5\n",
        "selector = SelectKBest(score_func=f_classif, k=5)\n",
        "\n",
        "# Fit and transform the selector on your data\n",
        "X_new = selector.fit_transform(X, y)\n",
        "```\n",
        "\n",
        "You can choose the scoring function that best fits your specific classification or regression problem to perform feature selection effectively."
      ],
      "metadata": {
        "id": "HPhshWYe8ab0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 3 - Recursive Feature Elimination (RFE):"
      ],
      "metadata": {
        "id": "91fxcDn08pOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset as an example\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Create a DataFrame from the dataset\n",
        "df = pd.DataFrame(data=X, columns=data.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"Original Dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Instantiate a RandomForestClassifier as the estimator\n",
        "estimator = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Instantiate the RFE selector with the estimator and the desired number of features to select (e.g., 5)\n",
        "selector = RFE(estimator, n_features_to_select=5)\n",
        "\n",
        "# Fit the selector on the training data\n",
        "selector = selector.fit(X_train, y_train)\n",
        "\n",
        "# Get the ranking of features (1 indicates selected, 0 indicates not selected)\n",
        "feature_ranking = selector.support_\n",
        "print(\"------------------\")\n",
        "print(feature_ranking)\n",
        "print(\"------------------\")\n",
        "\n",
        "# Get the indices of the selected features\n",
        "selected_indices = np.where(feature_ranking)[0]\n",
        "\n",
        "# Create a DataFrame with the selected features\n",
        "df_selected = df.iloc[:, selected_indices]\n",
        "\n",
        "\n",
        "# Display the selected features\n",
        "print(\"\\nSelected Features:\")\n",
        "print(df_selected.head())\n",
        "\n",
        "# Train a RandomForestClassifier on the selected features\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train[:, feature_ranking], y_train)\n",
        "\n",
        "# Make predictions on the test data using the selected features\n",
        "y_pred = clf.predict(X_test[:, feature_ranking])\n",
        "\n",
        "# Calculate and display the accuracy of the model on the test data\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nAccuracy on Test Data:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XROgHepw57gB",
        "outputId": "e87040d1-ea7f-4d97-b333-acfaaacf8585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "3           0.28390          0.2414              0.10520         0.2597   \n",
            "4           0.13280          0.1980              0.10430         0.1809   \n",
            "\n",
            "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
            "0                 0.07871  ...          17.33           184.60      2019.0   \n",
            "1                 0.05667  ...          23.41           158.80      1956.0   \n",
            "2                 0.05999  ...          25.53           152.50      1709.0   \n",
            "3                 0.09744  ...          26.50            98.87       567.7   \n",
            "4                 0.05883  ...          16.67           152.20      1575.0   \n",
            "\n",
            "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
            "0            0.1622             0.6656           0.7119                0.2654   \n",
            "1            0.1238             0.1866           0.2416                0.1860   \n",
            "2            0.1444             0.4245           0.4504                0.2430   \n",
            "3            0.2098             0.8663           0.6869                0.2575   \n",
            "4            0.1374             0.2050           0.4000                0.1625   \n",
            "\n",
            "   worst symmetry  worst fractal dimension  target  \n",
            "0          0.4601                  0.11890       0  \n",
            "1          0.2750                  0.08902       0  \n",
            "2          0.3613                  0.08758       0  \n",
            "3          0.6638                  0.17300       0  \n",
            "4          0.2364                  0.07678       0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "------------------\n",
            "[False False False False False False False  True False False False False\n",
            " False False False False False False False False  True False  True  True\n",
            " False False False  True False False]\n",
            "------------------\n",
            "\n",
            "Selected Features:\n",
            "   mean concave points  worst radius  worst perimeter  worst area  \\\n",
            "0              0.14710         25.38           184.60      2019.0   \n",
            "1              0.07017         24.99           158.80      1956.0   \n",
            "2              0.12790         23.57           152.50      1709.0   \n",
            "3              0.10520         14.91            98.87       567.7   \n",
            "4              0.10430         22.54           152.20      1575.0   \n",
            "\n",
            "   worst concave points  \n",
            "0                0.2654  \n",
            "1                0.1860  \n",
            "2                0.2430  \n",
            "3                0.2575  \n",
            "4                0.1625  \n",
            "\n",
            "Accuracy on Test Data: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Key Logic in the Code Explained\n",
        "\n",
        "`np.where(selector.support_)[0]` is a NumPy operation used to obtain the indices of selected features based on the boolean mask created by `selector.support_`. Let's break down how it works step by step:\n",
        "\n",
        "1. `selector.support_` is a boolean mask created by the Recursive Feature Elimination (RFE) selector. It has the same length as the number of features in your dataset. Each element of this mask is `True` if the corresponding feature is selected by RFE or `False` if it's not selected.\n",
        "\n",
        "2. `np.where(selector.support_)` returns a tuple containing the indices (positions) where the boolean mask is `True`. The result is a tuple because there could be multiple positions where `True` appears in the mask.\n",
        "\n",
        "3. `np.where(selector.support_)[0]` extracts the first element of the tuple, which contains the selected feature indices.\n",
        "\n",
        "Here's an example to illustrate this:\n",
        "\n",
        "Suppose you have a boolean mask `selector.support_` like this:\n",
        "\n",
        "```python\n",
        "selector.support_ = [True, False, True, False, True]\n",
        "```\n",
        "\n",
        "In this case, `True` corresponds to the selected features, and `False` corresponds to the non-selected features.\n",
        "\n",
        "- `np.where(selector.support_)` would return a tuple with the positions of `True` values:\n",
        "\n",
        "  ```\n",
        "  (array([0, 2, 4]),)\n",
        "  ```\n",
        "\n",
        "- Finally, `np.where(selector.support_)[0]` extracts the first element of the tuple, which gives you the selected feature indices:\n",
        "\n",
        "  ```\n",
        "  [0, 2, 4]\n",
        "  ```\n",
        "\n",
        "So, `np.where(selector.support_)[0]` returns a list of indices where the features are selected by RFE, allowing you to extract the columns (features) from your DataFrame that should be used for further analysis or modeling."
      ],
      "metadata": {
        "id": "R92IeLuRBHEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FAQ**\n",
        "\n",
        "What is the problem with above Feature Selection techniques that many developers use Feature Extraction Techniques like PCA ?"
      ],
      "metadata": {
        "id": "__2qNR05BXHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection techniques and feature extraction techniques like Principal Component Analysis (PCA) serve different purposes and have their own advantages and limitations. The choice between them depends on the specific characteristics of your data and the goals of your machine learning project. Here are some considerations for why developers might choose feature extraction techniques like PCA over feature selection techniques:\n",
        "\n",
        "1. **Dimensionality Reduction**: Feature extraction techniques like PCA are primarily used for dimensionality reduction. If you have a high-dimensional dataset with many features, reducing the dimensionality can help overcome issues related to the curse of dimensionality and computational complexity. Feature selection may not effectively reduce dimensionality to the same extent.\n",
        "\n",
        "2. **Linear Combinations**: PCA finds linear combinations of the original features that capture the most variance in the data. This can be valuable when there are multicollinearity issues (high correlation between features), as PCA can decorrelate the features and provide orthogonal components.\n",
        "\n",
        "3. **Unsupervised Approach**: PCA is an unsupervised technique and does not rely on target labels. It explores the inherent structure of the data based on covariance and eigenvalues. Feature selection methods, on the other hand, may take target labels into account, making PCA suitable for unsupervised learning tasks.\n",
        "\n",
        "4. **Noise Reduction**: PCA tends to reduce the influence of noisy features by emphasizing the directions (principal components) in the data with the most information. Feature selection techniques may or may not effectively handle noisy features.\n",
        "\n",
        "However, it's essential to consider the potential downsides of using feature extraction techniques like PCA:\n",
        "\n",
        "1. **Loss of Interpretability**: PCA transforms the original features into principal components, which may not have clear interpretations. Feature selection retains the original features, making it easier to interpret the importance of each feature.\n",
        "\n",
        "2. **Information Loss**: PCA may discard some information, as it focuses on capturing the most significant variance. Depending on the application, this information loss can be detrimental.\n",
        "\n",
        "3. **Non-linearity**: PCA assumes that the relationships between features are linear. If your data has non-linear relationships, PCA may not be the most effective technique.\n",
        "\n",
        "4. **Scalability**: PCA can be computationally expensive for very high-dimensional datasets, although there are techniques like Randomized PCA that can help mitigate this issue.\n",
        "\n",
        "In summary, both feature selection and feature extraction techniques have their places in machine learning, and the choice between them should be based on the specific characteristics of your data, the goals of your project, and the trade-offs you are willing to make in terms of interpretability and information preservation."
      ],
      "metadata": {
        "id": "Nh1wpAVHDUiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------\n",
        "\n",
        "Connect with the author of this Notebook - Rocky Jagtiani - [Here](https://www.linkedin.com/in/rocky-jagtiani-3b390649/)\n",
        "\n",
        "---------\n",
        "\n",
        "\n",
        "### Bonus NB - on Feature Extraction\n",
        "https://colab.research.google.com/drive/1gDWDKY2fMIZcassnz8ViWuOy00JIAAtd?usp=sharing"
      ],
      "metadata": {
        "id": "2oRL1u1WDfSt"
      }
    }
  ]
}